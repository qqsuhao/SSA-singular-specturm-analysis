<span id="mulu">目录</span>
>[第二章 SSA基础](#index2)
>>[2.1 基础算法](#index2.1)
>>>[2.1.1 算法描述](#index2.1.1)
>>>>[2.1.1.1 第一阶段：分解](#index2.1.1.1)
>>>>[2.1.1.2 第二阶段：重构](#index2.1.1.2)
>>>
>>>[2.1.2 对SSA基础算法中四个步骤的分析](#index2.1.2)
>>>>[2.1.2.1 嵌入](#index2.1.2.1)
>>>>[2.1.2.2 SVD分解](#index2.1.2.2)
>>>>[2.1.2.3 分组](#index2.1.2.3)



---
# <span id="index2">第二章 SSA基础</span> [目录](#mulu)
## <span id="index2.1"> 2.1 基础算法</span> [目录](#mulu)
### <span id="index2.1.1">2.1.1 算法描述</span> [目录](#mulu)

考虑一个长度为$N$的时间序列$\mathbb{X}=\mathbb{X}_{N}=\left(x_{1}, \ldots, x_{N}\right)$。假设$N>2$且$\mathbb{X}$是一个非零序列。用$L(1<L<N)$表示窗口长度，并且$K=N-L+1$。下面将讲述SSA的基础算法，其中包含两个阶段：分解和重构。

#### <span id="index2.1.1.1">2.1.1.1 第一阶段：分解</span> [目录](#mulu)
**第一步：嵌入**

我们将原始序列映射为长度为$L$的$K=N-L+1$个滞后向量，称其为L-滞后向量。
$$
X_{i}=\left(x_{i}, \ldots, x_{i+L-1}\right)^{\mathrm{T}} \quad(1 \leq i \leq K)
$$
由L-滞后向量组成L-轨迹矩阵：
<span id="eq2.1"></span>
$$
\mathbf{X}=\left[X_{1}: \ldots: X_{K}\right]=\left(x_{i j}\right)_{i, j=1}^{L, K}=\left(\begin{array}{llll}
x_{1} & x_{2} & x_{3} & \ldots x_{K} \\
x_{2} & x_{3} & x_{4} & \ldots x_{K+1} \\
x_{3} & x_{4} & x_{5} & \ldots x_{K+2} \\
\vdots & \vdots & \vdots & \ddots \\
x_{L} & x_{L+1} & x_{L+2} & \ldots x_{N}
\end{array}\right)  \tag{2.1} 
$$
滞后向量$X_{i}$是**轨迹矩阵**$\mathbf{X}$的列。$\mathbf{X}$的行和列都是原序列的子序列。$\mathbf{X}$中的位于$(i,j)$的元素$x_{ij}=x_{i+j-1}$，因此$\mathbf{X}$中反对角线的元素相等。（因此轨迹矩阵也被称称作**汉克尔矩阵**）公式[(2.1)](#eq2.1)定义了一个从时间序列到轨迹矩阵的一对一的映射。


**第二步：奇异值分解**
在这一步，我们对$\mathbf{X}$进行SVD分解。令$\mathbf{S}=\mathbf{X}\mathbf{X}^{T}$并且用$\lambda_{1}, \ldots, \lambda_{L}$表示$\mathbf{S}$的特征值，其中这些特征值按照从大到小的顺序排列。用$U_{1}, \ldots, U_{L}$表示对应的特征向量。用$d$表示矩阵$\mathbf{X}$的秩，一般情况下，如果时间序列是真实世界中获取到的，$d=L^*=min{L,K}$。记$V_{i}=\mathbf{X}^{\mathrm{T}} U_{i} / \sqrt{\lambda_{i}}(i=1, \ldots, d)$。轨迹矩阵$\mathbf{X}$可以被分解为：
<span id="eq2.2"></span>
$$
\mathbf{X}=\mathbf{X}_{1}+\ldots+\mathbf{X}_{d}  \tag{2.2}
$$
其中$\mathbf{X}_{i}=\sqrt{\lambda_{i}} U_{i} V_{i}^{\mathrm{T}}$。矩阵$\mathbf{X}_{i}$的秩为1，这样的矩阵被称作初等矩阵（elementary matrices），集合$\left(\sqrt{\lambda_{i}}, U_{i}, V_{i}\right)$被称作三特征（eigentriple）简记作ET。


#### <span id="index2.1.1.2">2.1.1.2 第二阶段：重构</span> [目录](#mulu)
**第三步：三特征分组**
一旦得到[(2.2)](#eq2.2)的分解结果，就把$d$个分解结果分配到$m$个不相交的子集中$I_{1}, \ldots, I_{m}$。令$I=\left\{i_{1}, \ldots, i_{p}\right\}$。分组$I$对应的分组结果$\mathbf{X}_{I}=\mathbf{X}_{i_1}+...+\mathbf{X}_{i_p}$。最终分组后的结果记为
<span id="eq2.3"></span>
$$
\mathbf{X}=\mathbf{X}_{I_{1}}+\ldots+\mathbf{X}_{I_{m}} \tag{2.3}
$$
挑选集合$I_{1}, \ldots, I_{m}$的过程称作三特征分组，如果$m=d$，即$I_{j}=\{j\}, j=1, \ldots, d$，则该分组成为初等分组。


**第四步：对角平均**
在这一步，我们需要把矩阵$\mathbf{X}_{I_{j}}$还原为时间序列。令$\mathbf{Y}$表示$L*K$的一个矩阵，记$L^*=min(L, K), K^*=max(L,K)，N=L+K-1$。令$y_{i j}^{*}=y_{i j}$如果$L<K$，否则$y_{i j}^{*}=y_{j i}$。使用以下公式将矩阵$\mathbf{Y}$还原为时间序列：
<span id="eq2.4"></span>
$$
y_{k}= \begin{cases}\frac{1}{k} \sum_{m=1}^{k} y_{m, k-m+1}^{*} & \text { for } 1 \leq k<L^{*}, \\ \frac{1}{L^{*}} \sum_{m=1}^{L^{*}} y_{m, k-m+1}^{*} & \text { for } L^{*} \leq k \leq K^{*}, \\ \frac{1}{N-k+1} \sum_{m=k-K^{*}+1}^{N-K^{*}+1} y_{m, k-m+1}^{*} & \text { for } K^{*}<k \leq N .\end{cases}
\tag{2.4}
$$
该公式对应着求取矩阵反对角线元素的平均值。

对$\mathbf{X}_{I_{k}}$使用对角平均可以得到重构序列$\tilde{\mathrm{X}}^{(k)}=\left(\tilde{x}_{1}^{(k)}, \ldots, \tilde{x}_{N}^{(k)}\right)$。最终，初始序列被分解为$m$成分：
<span id="eq2.5"></span>
$$
x_{n}=\sum_{k=1}^{m} \tilde{x}_{n}^{(k)} \quad(n=1,2, \ldots, N)    \tag{2.5}
$$
由初等分组得到的序列重构结果被称作初等重构序列（elementary reconstructed series）。

**Remark 2.1** 基础SSA算法可以自然扩展到复数时间序列：唯一的区别在于矩阵的转置要替换为复数的共轭转置。


### <span id="index2.1.2">2.1.2 对SSA基础算法中四个步骤的分析 [目录](#mulu)
SSA基础算法中的步骤需要一些说明。 在这个部分我们简要讨论所涉及的程序的含义。

#### <span id="index2.1.2.1">2.1.2.1 嵌入 [目录](#mulu)</span>
嵌入是一个把一维时间序列$\mathbb{X}=\left(x_{1}, \ldots,x_{N}\right)$使用向量$X_{i}=\left(x_{i}, \ldots, x_{i+L-1}\right)$转换为多维序列$X_{1}, \ldots, X_{K}$的过程，其中$K=N-L+1$。嵌入的参数是窗口长度$L$。注意轨迹矩阵[(2.1)](#eq2.1)具有明显的对称性。$\mathbb{X}^T$相当于是窗口长度为K的多维序列。

对于动力系统专家来说，一种常用的技术是获得所有成对的滞后向量$\mathbf{X}_i$和$\mathbf{X}_j$之间的经验，然后计算时间序列相关的维度。该维数与产生时间序列的动力系统的吸引子的分形维数有关。需要注意的是，在这种方法中，$L$必须相当小而$K$相当大。同样，在具有汉克尔矩阵结构的结构总最小二乘（Structural Total Least Squares）中，通常的做法是选择$L = r + 1$，其中$r$是近似矩阵的猜测秩。

在SSA中，窗口长度$L$应该足够大。特别是，$L$必须足够大，以便每个$L$滞后向量包含初始序列$\mathbb{X}=\left(x_{1}, \ldots,x_{N}\right)$的一个基础部分。$L$必须足够大时，才有可能将每个$L$滞后向量$\mathbf{X}_i$视为一个单独的序列。


#### <span id="index2.1.2.2">2.1.2.2 SVD分解 [目录](#mulu)</span>
SVD分解可以使用不同的术语被描述，并且可以被用于不同的目的。让我们从SVD一般的特点开始讲解，这些特点对于理解SSA十分重要。

正如我们已经提到过的，SVD分解可以对任意的维度为$L*K$的矩阵$\mathbf{X}=\left[X_{1}: \ldots . X_{K}\right]$进行分解，分解形式如下：
<span id="eq2.6"></span>
$$
\mathbf{X}=\sum_{i=1}^{d} \sqrt{\lambda_{i}} U_{i} V_{i}^{\mathrm{T}} \tag{2.6}
$$
其中$\lambda_i(i=1,...,L)$是矩阵$\mathbf{S}=\mathbf{X X}^{\mathrm{T}}$的特征值，并且按照递减的顺序进行排列。$d=max\{i, such that \lambda_i>0\}=rank(\mathbf{X})$，$\left\{U_{1}, \ldots, U_{d}\right\}$是$\mathbf{S}$的政教特征向量，并且$V_{i}=\mathbf{X}^{\mathrm{T}} U_{i} / \sqrt{\lambda_{i}}$。

标准的SVD术语称$\sqrt{\lambda_{i}}$为奇异值；$U_{i}$和$V_{i}$为矩阵$\mathbf{X}$的左奇异向量和右奇异向量。如果我们定义$\mathbf{X}_i=\sqrt{\lambda_{i}} U_{i} V_{i}^{\mathrm{T}}$，那么公式[(2.6)](#eq2.6)可以被写作公式[(2.2)](#eq2.2)的形式。

如果所有特征向量的重数为1，那么展开式[(2.2)](#eq2.2)是唯一确定的。否则，如果至少有一个特征值的重数大于1，那么在选择相应的特征向量时就有了自由度。我们应该假设以某种方式选择特征向量并且选择是固定的。

公式[(2.6)](#eq2.6)说明SVD分解具有以下的对称性：$V_{1}, \ldots, V_{d}$构成了矩阵$\mathbf{X}^{\mathrm{T}} \mathbf{X}$的特征正交系统。注意到轨迹矩阵的行和列都是原时间序列的子序列，因此左右奇异向量都具有时间结构，所以这些奇异向量也可以看做时间序列。

SVD分解具有很多特征，其中一个如下：在所有秩$r<d$的矩阵$\mathbf{X}^{(r)}$中，矩阵$\sum_{i=1}^{r} \mathbf{X}_{i}$给出了对轨迹矩阵$\mathbf{X}$的最优估计，即$\left\|\mathbf{X}-\mathbf{X}^{(r)}\right\|_{\mathrm{F}}$最小。其中$F$表示矩阵的Frobenius范数，一个矩阵$\mathbf{Y}$的Frobenius范数为$\|\mathbf{Y}\|_{F}=\sqrt{\langle\mathbf{Y}, \mathbf{Y}\rangle_{F}}$。其中两个矩阵$\mathbf{Y}=\left\{y_{i j}\right\}_{i, j=1}^{q, s}$和$\mathbf{Z}=\left\{z_{i j}\right\}_{i, j=1}^{q, s}$的内积定义为：
$$
\langle\mathbf{Y}, \mathbf{Z}\rangle_{\mathrm{F}}=\sum_{i, j=1}^{q, s} y_{i j} z_{i j}
$$
对于向量而言，Frobenius范数等同于传统的欧几里得范数。

注意到$\|\mathbf{X}\|_{\mathrm{F}}^{2}=\sum_{i=1}^{d} \lambda_{i}$并且$\lambda_{i}=\left\|\mathbf{X}_{i}\right\|_{\mathrm{F}}^{2}$，因此我们考虑将$\lambda_{i} /\|\mathbf{X}\|_{\mathrm{F}}^{2}$作为度量矩阵$\mathbf{X}_{i}$在整个轨迹矩阵$\mathbf{X}$中所占的比重。最终可以使用$\sum_{i=1}^{r} \lambda_{i} /\|\mathbf{X}\|_{\mathrm{F}}^{2}$作为轨迹矩阵的最优近似的特征指标。而且，如果$\lambda_{r} \neq \lambda_{r+1}$，可以把$\sum_{i=r+1}^{d} \lambda_{i}$作为轨迹矩阵与其最优估计之间的误差距离。

现在我们考虑把轨迹矩阵$\mathbf{X}$看做是一系列L延时的向量。用$\mathcal{X}^{(L)} \subset \mathrm{R}^{L}$表示向量$X_{1}, \ldots, X_{K}$组成的线性空间。我们把这个空间称作时间序列$\mathbb{X}$的L-轨迹空间。为了强调序列$\mathbb{X}$的作用，我们把这个空间记作$\mathcal{X}^{(L)}(\mathbb{X})$。公式[(2.6)](#eq2.6)表明$\mathscr{U}=\left(U_{1}, \ldots, U_{d}\right)$是这个轨迹空间的一组正交基向量。

令$Z_{i}=\sqrt{\lambda_{i}} V_{i}, i=1, \ldots, d$，公式[(2.6)](#eq2.6)可以被写作$\mathbf{X}=\sum_{i=1}^{d} U_{i} Z_{i}^{\mathrm{T}}$，并且对于延时向量$X_j$我们有$X_{j}=\sum_{i=1}^{d} z_{j i} U_{i}$，其中$z_{ji}$是向量$Z_i$中的元素。这意味着向量$Z_i$是由向量$X_{j}$在基向量$\mathscr{U}$下的第$i$个成分组成的。

让我们考虑转置的轨迹矩阵$\mathbf{X}^T$。引入$Y_{i}=\sqrt{\lambda_{i}} U_{i}$，我们可以得到展开式$\mathbf{X}^{\mathrm{T}}=\sum_{i=1}^{d} V_{i} Y_{i}^{\mathrm{T}}$，该公式对应着K-延时向量在正交基$V_{1}, \ldots, V_{d}$下的表示。至此，SVD分解引出了关于轨迹矩阵$\mathbf{X}$的两种几何描述。

上述关于SVD特性的描述可以用L延时向量的多变量几何语言进行描述，如下：令$r<d$，那么在整个$r$维子空间$\mathcal{L}_{r}$中，由$U_{1}, \ldots, U_{r}$张成的子空间可以最好得估计原空间。也就是说，最小化$\sum_{i=1}^{K} \operatorname{dist}^{2}\left(X_{i}, \mathcal{L}_{r}\right)$得到结果是$\mathcal{L}_{r}=\operatorname{span}\left(U_{1}, \ldots, U_{r}\right)$，而$\sum_{i=1}^{r} \lambda_{i} / \sum_{i=1}^{d} \lambda_{i}$是这些延时向量的$r$维最佳估计的特征。

SVD的另一个特征与特征向量$U_{1}, \ldots, U_{d}$有关。具体地，第一个特征向量$U_1$决定了延时向量的投影变化最大的方向。后续的每一个特征向量都与之前的特征向量正交，并且延时向量在该方向上的投影变化也是最大的。因此，很自然地将第$i$个特征向量所表示的方向称作第$i$个主方向。注意到，矩阵$\mathbf{X}_{i}=U_{i} Z_{i}^{\mathrm{T}}$是延时向量在第$i$个主方向投影后构成的。

这种对于SVD分解的理解角度引出了以下的术语。我们称向量$U_i$为第$i$个主特征向量，向量$V_i$和$Z_{i}=\sqrt{\lambda_{i}} V_{i}$被称作第$i$个音字向量和第$i$个主成分。

**Remark2.2**：SSA基础算法中所使用的SVD分解类似于经典的多变量分析中的主成分分析（PCA）和平稳时间序列分析中的KL变换。然而，SSA中的SVD方法利用了轨迹矩阵具有汉克尔结构的特点。事实上，轨迹矩阵的行和列都是原始序列的子序列并具有相同的时域意义，而PCA和KL变换并非如此。

**Remark 2.3**：一般地，轨迹空间中的任何一组正交基$P_{1}, \ldots, P_{d}$都可以替代通过SVD分解得到的正交基$U_{1}, \ldots, U_{d}$。在这种情况下，公式[(2.2)](#eq2.2)可以被替换为$\mathbf{X}_{i}=P_{i} Q_{i}^{\mathrm{T}}$，其中$Q_{i}=\mathbf{X}^{\mathrm{T}} P_{i}$。一种可以替换正交基的例子是，在拓普利兹SSA中的自相关矩阵中的正交基，见章节2.5.3。另一个例子可以在独立成分分析（ICA）和因子分析中看到，见章节2.5.4。

关于SVD进一步的讨论以及使用见章节2.5.7。


#### <span id=index2.1.2.3>2.1.2.3 分组</span> [目录](#mulu)



